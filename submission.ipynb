{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac91ca9f",
   "metadata": {},
   "source": [
    "# Coin Classifier Model\n",
    "This model is designed for recognition of different currencies, their denominations and country\n",
    "\n",
    "# Outline\n",
    "1. [Import Packages](#1---import-packages)\n",
    "2. [Hyperparameters](#2---hyperparameters)\n",
    "3. [Loading the Dataset](#3---loading-the-dataset)\n",
    "   - [Encoding Labels](#31---encoding-labels)\n",
    "   - [Train/Val split](#32---splitting-the-data-into-test-and-validation-sets)\n",
    "   - [Image loading](#33---image-preprocessing-and-loading)\n",
    "4. [Data Augmentation](#4---data-augmentation)\n",
    "5. [Data Loader](#5---dataloader)\n",
    "6. [Model](#6---load-pre-trained-efficientnet-b0-model)\n",
    "7. [Pipeline](#7---setup-pipeline)\n",
    "   - [Validation](#71---validation-pipeline)\n",
    "   - [Training](#72---training-pipeline)\n",
    "8. [Evaluation](#8---evaluation)\n",
    "9. [Inference](#9---inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055cbb9",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af1c84",
   "metadata": {},
   "source": [
    "The following packages are used:\n",
    "- `numpy` for  scientific computation in python\n",
    "- `torch` and `sklearn` for defining the model architecture\n",
    "- `os` and `pandas` for data manipulation\n",
    "- `PIL` for image manipulation\n",
    "- `matplotlib` and `seaborn` for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timm\n",
    "import warnings\n",
    "import platform\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241f603",
   "metadata": {},
   "source": [
    "### Choose your device type\n",
    "Only run the block for your device type\n",
    "\n",
    "- (Note: If your device doesn't match either of the blocks, running any one of them is fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff00b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU (For Macbook Silicon)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e43893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU (For Nvidia-based systems)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec6534",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c238f",
   "metadata": {},
   "source": [
    "Hyperparameter Choices\n",
    "- Batch Size: Set to 32 to allow fast training without overloading memory. This size provides a good balance between convergence stability and training speed.\n",
    "- Learning Rate: Set to 1e-4 to ensure gradual learning and prevent overshooting the minimum, especially since we’re using a pretrained model.\n",
    "- Epochs: 10 epochs used, depending on validation performance, to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"train.csv\"\n",
    "TRAIN_IMG_DIR = \"./train\"\n",
    "TEST_CSV = \"test.csv\"\n",
    "TEST_IMG_DIR = \"./test\"\n",
    "OUTLINE_CSV = \"sample_submission.csv\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbda229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set num_workers based on OS\n",
    "if platform.system() == \"Darwin\":  # macOS\n",
    "    NUM_WORKERS = 0\n",
    "else:\n",
    "    NUM_WORKERS = 2  # or 4, depending on the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea67bd",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 3 - Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e7f76",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### 3.1 - Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_CSV)\n",
    "label_to_idx = {label: idx for idx, label in enumerate(sorted(df[\"Class\"].unique()))}\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "df[\"label\"] = df[\"Class\"].map(label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffaca1",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 - Splitting the data into test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, stratify=df[\"label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf28c68",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 - Image preprocessing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoinDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.supported_exts = ['.jpg', '.jpeg', '.png', '.webp']\n",
    "        self.label2idx = {label: idx for idx, label in enumerate(sorted(self.dataframe['Class'].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = str(self.dataframe.iloc[idx, 0])\n",
    "        label_str = self.dataframe.iloc[idx, 1]\n",
    "        label = torch.tensor(self.label2idx[label_str])\n",
    "\n",
    "        img_path = None\n",
    "        for ext in self.supported_exts:\n",
    "            possible_path = os.path.join(self.img_dir, img_id + ext)\n",
    "            if os.path.exists(possible_path):\n",
    "                img_path = possible_path\n",
    "                break\n",
    "\n",
    "        if img_path is None:\n",
    "            warnings.warn(f\"Image not found for ID {img_id}, returning dummy.\")\n",
    "            return torch.zeros(3, 224, 224), label\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            warnings.warn(f\"Corrupted image: {img_path}, returning dummy.\")\n",
    "            return torch.zeros(3, 224, 224), label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2b49f",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762368d",
   "metadata": {},
   "source": [
    "Without data augmentation, the EfficientNet model achieved a validation accuracy of around 65–70%. However, after incorporating data augmentation techniques, the accuracy significantly improved to approximately 85–90%, representing a substantial performance boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945de96f",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CoinDataset(train_df, TRAIN_IMG_DIR, transform=train_transforms)\n",
    "val_dataset = CoinDataset(val_df, TRAIN_IMG_DIR, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe003e2",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Load pre-trained EfficientNet B0 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6ace6",
   "metadata": {},
   "source": [
    "<a name=\"6.1\"></a>\n",
    "### 6.1 - Setup Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b86fae",
   "metadata": {},
   "source": [
    "Initially, I implemented a custom Convolutional Neural Network (CNN) from scratch using layers such as Conv2D, MaxPooling2D, Dropout, and BatchNormalization. However, due to the large size and complexity of the dataset, the model took an extremely long time to train—approximately 5 to 6 hours—and achieved a very low validation accuracy of only 10–15%.\n",
    "\n",
    "In my next approach, I leveraged transfer learning by fine-tuning a pre-trained ResNet50 model trained on the ImageNet dataset. While this model was somewhat faster to train (around 2 to 3 hours), it still failed to produce a significant improvement, yielding a validation accuracy of only 25–30%.\n",
    "\n",
    "After extensive research and experimentation, I discovered that EfficientNetB0 offered a more optimal trade-off between performance and efficiency. By fine-tuning a pre-trained EfficientNetB0 model on my coin classification dataset, I was able to reduce training time significantly to just 30 minutes. More importantly, the model achieved a substantial boost in validation accuracy, reaching around 85–90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ea406",
   "metadata": {},
   "source": [
    "- Note : All timings listed here are timings using the Apple Silicon M3 Pro chip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2abef",
   "metadata": {},
   "source": [
    "From my research, I found that EfficientNet is a strong choice for the backbone model because it strikes a great balance between accuracy and computational efficiency. It uses a compound scaling method that adjusts depth, width, and resolution in a structured way, which makes it especially effective for fine-grained tasks like coin classification. On top of that, using a pretrained EfficientNet model on ImageNet boosts performance significantly through transfer learning — even when the dataset is relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_to_idx)\n",
    "model = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac0349",
   "metadata": {},
   "source": [
    "<a name=\"6.2\"></a>\n",
    "### 6.2 - Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650de09b",
   "metadata": {},
   "source": [
    "- Loss Function: CrossEntropyLoss is used for multi-class classification, which is standard for categorical labels.\n",
    "- Optimizer: Adam optimizer was chosen for its adaptive learning capabilities and fast convergence on deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4fcee",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## 7 - Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0.0\n",
    "best_model_path = 'model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65386ec",
   "metadata": {},
   "source": [
    "<a name=\"7.1\"></a>\n",
    "### 7.1 - Validation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    val_accuracy = correct / len(val_dataset)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c58e7",
   "metadata": {},
   "source": [
    "<a name=\"7.2\"></a>\n",
    "### 7.2 - Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb4804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_path = 'model.pth'\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(train_dataset)\n",
    "        print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}, Training Accuracy: {acc:.4f}\")\n",
    "\n",
    "        _, val_accuracy = validate()\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262bcc02",
   "metadata": {},
   "source": [
    "<a name=\"7.3\"></a>\n",
    "### 7.3 - Run training loop, while dealing with corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edaa0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98d548",
   "metadata": {},
   "source": [
    "<a name=\"7.4\"></a>\n",
    "### 7.4 - Save model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f7d91",
   "metadata": {},
   "source": [
    "- Challenges Faced During Training and How They Were Overcome:\n",
    "    - Long training times when building a model from scratch\n",
    "    - Low accuracy with ResNet50 despite longer training\n",
    "- Fixed by:\n",
    "    - Switching to EfficientNetB0 (smaller + better-performing model)\n",
    "    - Adding data augmentation to improve generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5f465",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## 8 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_eval(model, dataloader, class_names, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    return y_true, y_pred\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5d2b5",
   "metadata": {},
   "source": [
    "<a name=\"8.1\"></a>\n",
    "### 8.1 - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "\n",
    "y_true, y_pred = classification_report_eval(model, val_loader, list(idx_to_label.values()), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b173b3",
   "metadata": {},
   "source": [
    "<a name=\"8.2\"></a>\n",
    "### 8.2 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true, y_pred, list(idx_to_label.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb9438",
   "metadata": {},
   "source": [
    "This approach aims to strike a balance between accuracy, training efficiency, and practical deployment. EfficientNet, combined with well-tuned hyperparameters and error handling, provides a robust pipeline for coin classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65dd680",
   "metadata": {},
   "source": [
    "What I’d Improve with More Time/Data\n",
    "- Tune learning rate, batch size, and augmentation pipeline.\n",
    "- Collect more class-balanced data.\n",
    "- Try OCR feature extraction + CNN fusion for text-heavy coins.\n",
    "- Use Test-Time Augmentation (TTA) or ensembling to further boost accuracy.\n",
    "- Try larger EfficientNet variants (B2/B3)\n",
    "- Apply label smoothing or learning rate schedules\n",
    "- Increase dataset size or perform semi-supervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f68a47",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n",
    "## 9 - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1b088",
   "metadata": {},
   "source": [
    "<a name=\"9.1\"></a>\n",
    "### 9.1 - Test image loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCoinDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.supported_exts = ['.jpg', '.jpeg', '.png', '.webp']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = str(self.dataframe.iloc[idx, 0])\n",
    "        for ext in self.supported_exts:\n",
    "            img_path = os.path.join(self.img_dir, img_id + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(f\"Test image {img_id} not found. Returning dummy.\")\n",
    "            return torch.zeros(3, 224, 224), img_id\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            warnings.warn(f\"Corrupted test image {img_id}. Returning dummy.\")\n",
    "            return torch.zeros(3, 224, 224), img_id\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105f960",
   "metadata": {},
   "source": [
    "<a name=\"9.2\"></a>\n",
    "### 9.2 - Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a544c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Load test set\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "test_dataset = TestCoinDataset(test_df, TEST_IMG_DIR, transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Run inference\n",
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    for images, ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        for img_id, pred in zip(ids, preds):\n",
    "            label_str = idx_to_label[pred]\n",
    "            predictions[int(img_id)] = label_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c7765",
   "metadata": {},
   "source": [
    "<a name=\"9.3\"></a>\n",
    "### 9.3 - Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original sample\n",
    "submission_df = pd.read_csv(OUTLINE_CSV)\n",
    "\n",
    "# Replace placeholder with predictions\n",
    "submission_df[\"Class\"] = submission_df[\"Id\"].map(predictions).fillna(\"unknown\")\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
